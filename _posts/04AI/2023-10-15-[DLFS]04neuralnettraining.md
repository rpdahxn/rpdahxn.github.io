---
title: "[밑시딥] 4장 신경망 학습"
categories:
  - AI
toc: true
toc_sticky: true
toc_icon: "sticky-note"
use_math: true
comments: true
---
`DeepLearningFromScratch CHAPTER4 Neural Network Training`
<br/>

기계학습에서 찾고자 하는 것은 **범용적으로** 사용할 수 있는, 새로운 데이터에 대해서도 문제를 잘 풀어낼 수 있는 모델이다.     
따라서 데이터를 **훈련 데이터***training data*와 **시험 데이터***test data*로 나눠서 학습과 실험을 진행한다.       
<br/>

신경망 학습이란 훈련 데이터로부터 매개변수들의 적절한 값을 자동으로 획득하는 것이다.      
이 '적절한' 값이라는 것에 대한 지표가 필요하다.      
<br/>

# 손실 함수 
**손실 함수***loss function*는 신경망 학습의 지표다. 신경망의 성능(*좋고 나쁨*)을 나타낸다.      
- **오차제곱합***sum of squares for error, SSE*       
$$E = \frac{1}{2}\sum_{k}{(y_k-t_k)}^2$$ , (*$y_k$: 신경망이 추정한 값, $t_k$: 원-핫 인코딩된 정답 레이블, $k$: 데이터의 차원 수*)     
값이 작을수록 정답에 가까운 상태를 의미한다.        

- **교차 엔트로피 오차***cross entropy error, CEE*       
$$E = -\sum_{k}t_k\log{y_k}$$       
정답에 해당하는 인덱스의 원소만 1이고 나머지는 0인 $t_k$와 로그를 곱하는 것이기 때문에 결국 정답일 때의 출력이 전체 값을 결정한다.        
<br/>

### 미니배치 학습         
훈련 데이터가 60,000개인 MNIST 데이터셋의 경우 60,000개의 손실 함수를 모두 계산해야 한다.     
이처럼 데이터가 많다면 전체 데이터 중 일부(=미니배치)만 골라 **미니배치 학습**을 진행할 수 있다.    
<br/>

### 왜 손실 함수를 설정하는가     
왜 '정확도'라는 지표를 놔두고 '손실 함수의 값'을 사용하나      
**정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.**       
| 🙊 *이 부분 이해 못 함*      
<br/>

# 미분
미분은 한 순간의 변화량을 표시한 것.     
- **수치 미분***numerical difference*    
아주 작은 차분으로 미분하는 것       
$$\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}$$       

- **해석적 미분***analytic difference*     
수식을 전개해 미분하는 것, 예를 들어 $y=x^2$을 해석적으로 미분하면 $\frac{dy}{dx}=2x$     
수치 미분은 오차를 포함하지만 해석적 미분은 오차를 포함하지 않는다.     

- **편미분***partial derivative*     
변수가 여럿인 함수에 대한 미분      
변수 $x_0, x_1$로 이루어진 함수 $f$가 있을 때 $(\frac{\partial f}{\partial x_0},\frac{\partial f}{\partial x_1})$처럼 모든 변수의 편미분을 벡터로 정리한 것을 **기울기***gradient*라고 한다.     
<br/>

# 기울기
| 🙊       
**기울기**는 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향을 가리킨다.     
이러한 기울기의 특성을 이용해 손실 함수의 최솟값을 찾는 것이 **경사법**이다.    

이때 기울기의 방향이 반드시 최솟값을 가리키는 것은 아니다.(*극솟값: 국소적인 최솟값, 안장점: 방향에 따라 극댓값/극솟값이 되는 점*)    
하지만 그 방향으로 가야 함수의 값을 줄일 수 있다.      
<br/>

### 경사법(경사 하강법)    
현재 위치에서 기울기를 구하고 그 방향으로 이동한다. 이동한 곳에서도 마찬가지로 기울기를 구하고 그 기울어진 방향으로 이동한다.      
이 과정을 반복해서 함수의 값을 점차 줄여 나간다.       
<br/>
$$x_0 = x_0 -\eta\frac{\partial f}{\partial x_0}$$      
$\eta$는 매개변수 값을 얼마나 갱신할지를 결정하는 **학습률***learning rate*다.     
학습률 같은 매개변수를 **하이퍼파라미터**라고 한다.    

***하이퍼파라미터**hyper parameter     
학습을 통해 자동으로 획득되는 매개변수(가중치, 편향)와 달리 사람이 직접 설정해야 하는 매개변수.     
<U>이 하이퍼파라미터들은 후보 값들 중에서 가장 잘 학습하는 값으로 설정한다.</U>    
<br/>

# 학습 알고리즘 구현
학습 절차
- 1단계 : 미니배치   
훈련 데이터의 일부(=미니배치)를 무작위로 가져온다.   
- 2단계 : 기울기 산출    
미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.     
- 3단계 : 매개변수 갱신      
가중치 매개변수를 기울기 방향으로 갱신한다.       
- 4단계 : 반복     
1~3 단계를 반복한다.     
<br/>

***에포크**epoch     
1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수.     
예를 들어 훈련 데이터 10,000개를 100개의 미니배치로 학습한다면, 경사하강법 100회를 반복해야 모든 데이터를 소진한 게 된다. 이 경우 100회가 1에폭이 된다. 
<br/>

# Reference
📖 [밑바닥부터 시작하는 딥러닝 ](https://product.kyobobook.co.kr/detail/S000001057805)