---
title: "[밑시딥] 5장 오차역전파법"
categories:
  - AI
toc: true
toc_sticky: true
toc_icon: "sticky-note"
use_math: true
comments: true
---
`DeepLearningFromScratch CHAPTER5 Backpropagation`
<br/>

앞 장에서 신경망의 가중치 매개변수에 대한 손실 함수의 기울기를 수치 미분을 사용해 구했다. 수치 미분은 구현이 쉽지만 계산 시간이 오래 걸린다는 게 단점이다.     
가중치 매개변수의 기울기를 효율적으로 계산하는 방법이 **오차역전파법***backpropagation*이다.    
<br/>

| 🙊 *gpt한테 물어봤다.*
오차역전파법을 한마디로 설명하면?
인공 신경망에서 예측과 실제 값 사이의 오차를 계산하고, 이 오차를 이용해 가중치와 편향을 조정하여 모델을 학습시키는 방법입니다.

| 🙊
오차역전파법의 주요 단계
- 순전파 : 학습 데이터로 값을 예측한다.
- 오차 계산 : 예측된 출력과 실제 정답 사이의 오차를 손실 함수를 사용해 계산한다. 
- 역전파 : 오차를 각 뉴런에서 이전 뉴런으로 역방향으로 전파한다. 이때 Chain Rule을 이용하여 각 뉴런의 오차 기여도를 계산한다.
- 가중치 및 편향 업데이트 : 오차 역전파를 통해 얻은 미분 값들을 사용하여 가중치와 편향을 조정한다.       
<br/>

# 연쇄 법칙
연쇄 법칙은 합성 함수(*여러 함수로 구성된 함수*)의 미분에 대한 성질이며 정의는 다음과 같다.       
" 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다. "        
<br/> 

## Softmax-with-Loss 계층         
입력 데이터는 Affine 계층과 ReLU 계층을 통과하며 **변환**되고, 마지막 Softmax 계층에서 **정규화**된다.       
         
신경망은 **학습**과 **추론** 작업을 수행한다. 추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다. Softmax 앞의 Affine 계층의 출력을 점수*score*라고 하는데, 신경망 추론에서는 가장 높은 점수만 알면 되기 때문에 Softmax 계층은 필요하지 않다. 반면, 신경망을 학습할 때는 Softmax 계층이 필요하다. 
<br/>

| 🙊
'소프트맥스 함수'의 손실 함수로 '교차 엔트로피 오차'를 사용하면 역전파가 $$(y_1-t_1, y_2-t_2, y_3-t_3)$$으로 말끔히 떨어진다. *($$y_n$$ : 출력값, $$t_n$$ : 정답 레이블)* <U>이 말끔함은 교차 엔트로피 오차 함수가 그렇게 설계되었기 때문이다.</U> 마찬가지로 회귀의 출력층에 사용하는 '항등 함수'의 손실 함수로 '오차제곱합'을 사용하면 역전파의 결과가 말끔히 떨어진다. 
<br/>

# 정리
- 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다. (오차역전파법)
<br/>

# Reference
📖 [밑바닥부터 시작하는 딥러닝 ](https://product.kyobobook.co.kr/detail/S000001057805)