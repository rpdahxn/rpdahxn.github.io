---
title: "[밑시딥] 3장 신경망"
categories:
  - AI
toc: true
toc_sticky: true
toc_icon: "sticky-note"
use_math: true
comments: true
---
`DeepLearningFromScratch CHAPTER3 Neural Network`
<br/>
 
# 퍼셉트론에서 신경망으로
적절한 가중치 값을 사람이 **수동으로** 설정해야 하는 퍼셉트론과 달리 신경망은 가중치 매개변수의 적절한 값을 데이터로부터 **자동으로 학습**한다.     
퍼셉트론에서 신경망으로 넘어가면서 **활성화 함수***activation function* 개념이 나온다.      
<br/>

# 활성화 함수
활성화 함수란 입력 신호의 총합을 출력 신호로 변환하는 함수를 가리키며 그 종류는 다양하다.       
일반적으로 **단층** 네트워크에서 **계단 함수**를 활성화 함수로 사용한 모델을 **단순 퍼셉트론**, **여러 층**으로 구성되고 시그모이드 함수 등의 **매끈한 활성화 함수**를 사용하는 네트워크를 **신경망**(다층 퍼셉트론)이라 한다.      
    
활성화 함수들   
- 시그모이드 함수, 계단 함수        
입력이 중요할수록 큰 값을 출력하며 그 값은 0에서 1사이에서만 존재      
- ReLU 함수       
입력이 0을 넘으면 그 값을 그대로 출력하고, 0이하이면 0을 출력     
<br/>

# 왜 비선형 함수?
신경망에서는 활성화 함수로 선형 함수가 아닌 비선형 함수를 사용해야 한다.     
**선형 함수를 사용하면 신경망의 층을 깊게 하는 것의 의미가 없어진다.**     
**선형 함수를 사용할 경우 신경망을 여러 층으로 구성하는 이점을 살릴 수 없다.**     
<br/>

| 🙊 잘 몰랐던 개념; *3x2 행렬이란 처음 *차원에는 원소가 3개, 다음 *차원에는 원소가 2개 있다는 의미이다.*
<br/>

# 다차원 배열
넘파이의 다차원 배열을 통해 신경망을 효율적으로 구현할 수 있다.    
행렬곱 연산을 이용한다.      
<br/>

# 분류 / 회귀 문제
어떤 문제냐에 따라 신경망의 출력층에서 사용하는 활성화 함수가 달라진다.    
       
- **항등 함수***identity function*      
일반적으로 회귀 문제에서 사용한다.      
입력을 그대로 출력한다.     

- **소프트맥스 함수***softmax function*      
일반적으로 분류 문제에서 사용한다.     
$$y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}exp(a_i)}$$     
소프트맥스 함수의 출력은 **'확률'**로 해석할 수 있다.     
<br/>

# 배치 처리
입력 데이터를 묶은 것을 가리킨다. 배치 단위로 추론을 진행하면 결과를 훨씬 빠르게 얻을 수 있다.
<br/>

# Reference
📖 [밑바닥부터 시작하는 딥러닝 ](https://product.kyobobook.co.kr/detail/S000001057805)